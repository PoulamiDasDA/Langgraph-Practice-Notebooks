{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27308b8",
   "metadata": {},
   "source": [
    "# Hill Model Marketing Agent with MLflow Tracking\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Hill Model Fitting** for single-channel marketing spend\n",
    "2. **MLflow Experiment Tracking** for parameters, metrics, and artifacts\n",
    "3. **LangGraph Agent** with MLflow tracing for predictions\n",
    "\n",
    "**MLflow tracks:**\n",
    "- Model parameters (Œ±, Œ≤, Œ∏)\n",
    "- Performance metrics (R¬≤, MAE, RMSE)\n",
    "- Training data\n",
    "- Model artifacts\n",
    "- Agent prediction traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ee21d",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15828275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# LangGraph and LangChain\n",
    "from typing import Annotated, Literal, Optional\n",
    "from operator import add\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Command\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úì Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878819c",
   "metadata": {},
   "source": [
    "## Configure MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70bb0c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MLflow experiment: Hill_Model_Marketing_Agent\n",
      "‚úì Tracking URI: ./mlruns\n"
     ]
    }
   ],
   "source": [
    "# Set MLflow tracking URI (local or remote)\n",
    "mlflow.set_tracking_uri(\"./mlruns\")  # Local tracking\n",
    "# For remote: mlflow.set_tracking_uri(\"databricks\") or \"http://mlflow-server:5000\"\n",
    "\n",
    "# Set experiment name\n",
    "experiment_name = \"Hill_Model_Marketing_Agent\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"‚úì MLflow experiment: {experiment_name}\")\n",
    "print(f\"‚úì Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e9158",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fec428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   Digital_Spend  Revenue\n",
      "0             10       95\n",
      "1             20      150\n",
      "2             30      205\n",
      "3             40      250\n",
      "4             50      285\n",
      "\n",
      "Shape: (12, 2)\n"
     ]
    }
   ],
   "source": [
    "# Historical marketing data (spend ‚Üí revenue in $k)\n",
    "df = pd.DataFrame({\n",
    "    \"Digital_Spend\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120],\n",
    "    \"Revenue\": [95, 150, 205, 250, 285, 305, 318, 325, 330, 334, 336, 337]\n",
    "})\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e30b2",
   "metadata": {},
   "source": [
    "## Define Hill Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3affccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Hill function defined\n"
     ]
    }
   ],
   "source": [
    "def hill(x, alpha, beta, theta):\n",
    "    \"\"\"\n",
    "    Hill transformation for marketing saturation:\n",
    "    y = Œ± √ó x^Œ≤ / (x^Œ≤ + Œ∏^Œ≤)\n",
    "    \n",
    "    Parameters:\n",
    "    - x: spend (in $k)\n",
    "    - alpha: plateau (max revenue contribution in $k)\n",
    "    - beta: steepness (curve shape, unitless)\n",
    "    - theta: half-saturation spend (in $k)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = np.clip(x, 1e-9, None)  # guard against zero\n",
    "    return alpha * (x**beta) / (x**beta + theta**beta)\n",
    "\n",
    "print(\"‚úì Hill function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce7438",
   "metadata": {},
   "source": [
    "## Train Hill Model with MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"hill_model_training\") as run:\n",
    "    \n",
    "    # Log dataset info\n",
    "    mlflow.log_param(\"n_samples\", len(df))\n",
    "    mlflow.log_param(\"channel\", \"Digital\")\n",
    "    mlflow.log_param(\"model_type\", \"Hill_Saturation\")\n",
    "    \n",
    "    # Prepare data\n",
    "    x = df[\"Digital_Spend\"].to_numpy(float)\n",
    "    y = df[\"Revenue\"].to_numpy(float)\n",
    "    \n",
    "    # Scale to [0,1] for stable optimization\n",
    "    x_max, y_max = x.max(), y.max()\n",
    "    x_s, y_s = x / x_max, y / y_max\n",
    "    \n",
    "    mlflow.log_param(\"x_max\", x_max)\n",
    "    mlflow.log_param(\"y_max\", y_max)\n",
    "    \n",
    "    # Fit Hill curve\n",
    "    p0 = [1.05, 1.0, 0.5]  # Initial guesses (scaled)\n",
    "    bounds = ([0.0, 0.0, 0.0], [5.0, 10.0, 5.0])\n",
    "    \n",
    "    mlflow.log_param(\"p0_alpha\", p0[0])\n",
    "    mlflow.log_param(\"p0_beta\", p0[1])\n",
    "    mlflow.log_param(\"p0_theta\", p0[2])\n",
    "    \n",
    "    params_s, _ = curve_fit(hill, x_s, y_s, p0=p0, bounds=bounds, maxfev=50000)\n",
    "    alpha_s, beta, theta_s = params_s\n",
    "    \n",
    "    # Rescale parameters back to original units\n",
    "    alpha = alpha_s * y_max  # plateau in $k\n",
    "    theta = theta_s * x_max  # half-saturation in $k\n",
    "    \n",
    "    # Log fitted parameters\n",
    "    mlflow.log_param(\"alpha_plateau_k\", round(alpha, 2))\n",
    "    mlflow.log_param(\"beta_steepness\", round(beta, 3))\n",
    "    mlflow.log_param(\"theta_halfsat_k\", round(theta, 2))\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = hill(x, alpha, beta, theta)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    \n",
    "    # Log training data as artifact\n",
    "    train_data_path = \"training_data.csv\"\n",
    "    df.to_csv(train_data_path, index=False)\n",
    "    mlflow.log_artifact(train_data_path)\n",
    "    \n",
    "    # Create and log visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x_line = np.linspace(0, x.max() * 1.2, 200)\n",
    "    y_line = hill(x_line, alpha, beta, theta)\n",
    "    \n",
    "    plt.scatter(x, y, color='blue', s=80, label='Actual Data', zorder=3)\n",
    "    plt.plot(x_line, y_line, color='red', linewidth=2.5, label='Hill Model Fit')\n",
    "    plt.xlabel(\"Digital Spend ($k)\", fontsize=12)\n",
    "    plt.ylabel(\"Revenue ($k)\", fontsize=12)\n",
    "    plt.title(f\"Hill Model: R¬≤={r2:.3f}, MAE={mae:.1f}k\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = \"hill_curve_fit.png\"\n",
    "    plt.savefig(plot_path, dpi=150)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Log residuals plot\n",
    "    residuals = y - y_pred\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x, residuals, alpha=0.7)\n",
    "    plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.xlabel(\"Digital Spend ($k)\")\n",
    "    plt.ylabel(\"Residual ($k)\")\n",
    "    plt.title(\"Residuals vs Spend\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(residuals, bins=8, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel(\"Residual ($k)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Residuals Distribution\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    residuals_path = \"residuals_analysis.png\"\n",
    "    plt.savefig(residuals_path, dpi=150)\n",
    "    mlflow.log_artifact(residuals_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Store model parameters globally for agent use\n",
    "    global MODEL_PARAMS\n",
    "    MODEL_PARAMS = {\n",
    "        'alpha': alpha,\n",
    "        'beta': beta,\n",
    "        'theta': theta\n",
    "    }\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MLflow Training Summary\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"\\nFitted Parameters:\")\n",
    "    print(f\"  Œ± (plateau):        {alpha:.2f}k\")\n",
    "    print(f\"  Œ≤ (steepness):      {beta:.3f}\")\n",
    "    print(f\"  Œ∏ (half-sat spend): {theta:.2f}k\")\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  R¬≤ Score:  {r2:.4f}\")\n",
    "    print(f\"  MAE:       {mae:.2f}k\")\n",
    "    print(f\"  RMSE:      {rmse:.2f}k\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cd41a",
   "metadata": {},
   "source": [
    "## Define Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d037a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_revenue(spend):\n",
    "    \"\"\"\n",
    "    Predict revenue for a given spend using fitted Hill model.\n",
    "    \n",
    "    Args:\n",
    "        spend: Digital spend in $k\n",
    "    \n",
    "    Returns:\n",
    "        Predicted revenue in $k\n",
    "    \"\"\"\n",
    "    return hill(spend, MODEL_PARAMS['alpha'], MODEL_PARAMS['beta'], MODEL_PARAMS['theta'])\n",
    "\n",
    "# Test predictions\n",
    "print(\"Sample Predictions:\")\n",
    "for s in [30, 60, 100, 140]:\n",
    "    print(f\"  Spend ${s}k ‚Üí Revenue ${predict_revenue(s):.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af4327",
   "metadata": {},
   "source": [
    "## Setup Azure OpenAI (Update with your credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"your-api-key-here\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://your-resource.openai.azure.com/\")\n",
    "\n",
    "routing_llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"‚úì Azure OpenAI configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d004e5",
   "metadata": {},
   "source": [
    "## Build LangGraph Agent with MLflow Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent state\n",
    "class AgentState(MessagesState):\n",
    "    next: Optional[str] = None\n",
    "    routed_intent: Annotated[list[str], add]\n",
    "    spend_k: Optional[float] = None\n",
    "\n",
    "# Define routing decision schema\n",
    "class RouteDecision(BaseModel):\n",
    "    target: Literal[\"Predictor\", \"FINISH\"]\n",
    "    reason: str\n",
    "    spend_k: Optional[float] = None  # spend in $k\n",
    "\n",
    "# Routing prompt\n",
    "routing_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\n",
    "        content=(\n",
    "            \"You supervise a marketing assistant.\\n\"\n",
    "            \"Respond with JSON containing `target`, `reason`, and `spend_k`.\\n\"\n",
    "            \"`target` must be `Predictor` when the user asks for revenue based on digital spend, otherwise `FINISH`.\\n\"\n",
    "            \"`spend_k` must be the numeric digital spend in thousands of dollars.\\n\"\n",
    "            \"Examples:\\n\"\n",
    "            \"- '10k' -> 10.0\\n\"\n",
    "            \"- '10,000 dollars' -> 10.0\\n\"\n",
    "            \"- '0.5 million' -> 500.0\\n\"\n",
    "            \"If no spend amount is stated, set `spend_k` to null.\"\n",
    "        )\n",
    "    ),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "routing_chain = routing_prompt | routing_llm.with_structured_output(RouteDecision)\n",
    "\n",
    "def _select_next(state: AgentState) -> str:\n",
    "    return state.get(\"next\", \"FINISH\")\n",
    "\n",
    "# Supervisor node\n",
    "def supervisor_node(state: AgentState):\n",
    "    last_human = next(msg for msg in reversed(state[\"messages\"]) if isinstance(msg, HumanMessage))\n",
    "    \n",
    "    try:\n",
    "        decision = routing_chain.invoke({\"query\": last_human.content})\n",
    "    except Exception as exc:\n",
    "        error_msg = AIMessage(content=f\"Routing failed: {exc}\")\n",
    "        return Command(\n",
    "            update={\n",
    "                \"next\": \"FINISH\",\n",
    "                \"messages\": [error_msg],\n",
    "                \"routed_intent\": [\"routing_error\"],\n",
    "                \"spend_k\": None,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    supervisor_note = AIMessage(\n",
    "        content=f\"Supervisor routing to {decision.target}: {decision.reason}\"\n",
    "    )\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"next\": decision.target,\n",
    "            \"spend_k\": decision.spend_k,\n",
    "            \"routed_intent\": [decision.reason],\n",
    "            \"messages\": [supervisor_note],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Predictor node with MLflow logging\n",
    "def predictor_node(state: AgentState):\n",
    "    spend_k = state.get(\"spend_k\")\n",
    "    \n",
    "    # Start MLflow run for prediction\n",
    "    with mlflow.start_run(run_name=\"agent_prediction\", nested=True):\n",
    "        \n",
    "        mlflow.log_param(\"input_spend_k\", spend_k)\n",
    "        \n",
    "        if spend_k is None:\n",
    "            reply = \"I couldn't detect a spend amount. Please specify an approximate digital spend (e.g., 10k).\"\n",
    "            mlflow.log_param(\"prediction_status\", \"missing_input\")\n",
    "            mlflow.log_metric(\"predicted_revenue_k\", 0.0)\n",
    "        else:\n",
    "            # Make prediction\n",
    "            est_revenue = predict_revenue(spend_k)\n",
    "            \n",
    "            # Log to MLflow\n",
    "            mlflow.log_metric(\"predicted_revenue_k\", est_revenue)\n",
    "            mlflow.log_param(\"prediction_status\", \"success\")\n",
    "            mlflow.log_param(\"model_alpha\", MODEL_PARAMS['alpha'])\n",
    "            mlflow.log_param(\"model_beta\", MODEL_PARAMS['beta'])\n",
    "            mlflow.log_param(\"model_theta\", MODEL_PARAMS['theta'])\n",
    "            \n",
    "            # Calculate ROI\n",
    "            roi = (est_revenue / spend_k - 1) * 100 if spend_k > 0 else 0\n",
    "            mlflow.log_metric(\"roi_percent\", roi)\n",
    "            \n",
    "            reply = (\n",
    "                f\"üìä **Marketing Prediction**\\n\\n\"\n",
    "                f\"üí∞ **Input:** ${spend_k:.1f}k digital spend\\n\"\n",
    "                f\"üìà **Predicted Revenue:** ${est_revenue:.1f}k\\n\"\n",
    "                f\"üéØ **ROI:** {roi:.1f}%\\n\\n\"\n",
    "                f\"_Model: Hill saturation curve (Œ±={MODEL_PARAMS['alpha']:.1f}, Œ≤={MODEL_PARAMS['beta']:.2f}, Œ∏={MODEL_PARAMS['theta']:.1f})_\"\n",
    "            )\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [AIMessage(content=reply)],\n",
    "            \"next\": \"FINISH\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Build the graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"supervisor\", supervisor_node)\n",
    "graph.add_node(\"Predictor\", predictor_node)\n",
    "graph.add_edge(START, \"supervisor\")\n",
    "graph.add_conditional_edges(\"supervisor\", _select_next, {\"Predictor\": \"Predictor\", \"FINISH\": END})\n",
    "graph.add_edge(\"Predictor\", END)\n",
    "\n",
    "marketing_agent = graph.compile()\n",
    "\n",
    "print(\"‚úì Marketing agent with MLflow tracing built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec70233",
   "metadata": {},
   "source": [
    "## Test Agent with MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95972fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What if I spend 50k on digital?\",\n",
    "    \"Predict revenue for 80,000 dollars digital spend\",\n",
    "    \"How much revenue if we spend 120k?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Agent with MLflow Tracking & Metrics\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Start MLflow run for agent testing\n",
    "with mlflow.start_run(run_name=f\"agent_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as test_run:\n",
    "    \n",
    "    # Log test metadata\n",
    "    mlflow.log_param(\"num_test_queries\", len(test_queries))\n",
    "    mlflow.log_param(\"test_type\", \"agent_execution\")\n",
    "    \n",
    "    # Metrics tracking\n",
    "    latencies = []\n",
    "    successes = 0\n",
    "    failures = 0\n",
    "    predicted_revenues = []\n",
    "    predicted_rois = []\n",
    "    \n",
    "    for idx, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîπ Query {idx}/{len(test_queries)}: {query}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Invoke agent with completely fresh state for each query\n",
    "            result = marketing_agent.invoke(\n",
    "                {\"messages\": [HumanMessage(content=query)]},\n",
    "                config={\"configurable\": {\"thread_id\": f\"test_{hash(query)}\"}}\n",
    "            )\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            # Give MLflow nested run time to complete\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Find the predictor's response\n",
    "            ai_messages = [msg for msg in result[\"messages\"] if isinstance(msg, AIMessage)]\n",
    "            prediction_messages = [msg for msg in ai_messages if \"Marketing Prediction\" in msg.content]\n",
    "            \n",
    "            if prediction_messages:\n",
    "                print(prediction_messages[0].content)\n",
    "                successes += 1\n",
    "                \n",
    "                # Extract metrics from response if available\n",
    "                # Parse the spend amount to calculate revenue and ROI\n",
    "                import re\n",
    "                spend_match = re.search(r'\\$(\\d+\\.?\\d*)k digital spend', prediction_messages[0].content)\n",
    "                revenue_match = re.search(r'Predicted Revenue:\\*\\* \\$(\\d+\\.?\\d*)k', prediction_messages[0].content)\n",
    "                roi_match = re.search(r'ROI:\\*\\* (-?\\d+\\.?\\d*)%', prediction_messages[0].content)\n",
    "                \n",
    "                if revenue_match:\n",
    "                    predicted_revenues.append(float(revenue_match.group(1)))\n",
    "                if roi_match:\n",
    "                    predicted_rois.append(float(roi_match.group(1)))\n",
    "                    \n",
    "            elif ai_messages:\n",
    "                print(max(ai_messages, key=lambda m: len(m.content)).content)\n",
    "                successes += 1\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No response generated\")\n",
    "                failures += 1\n",
    "            \n",
    "            # Log individual query metrics\n",
    "            mlflow.log_metric(f\"query_{idx}_latency_sec\", latency)\n",
    "            mlflow.log_metric(f\"query_{idx}_success\", 1 if prediction_messages else 0)\n",
    "            \n",
    "            print(f\"‚è±Ô∏è  Latency: {latency:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            latency = time.time() - start_time\n",
    "            latencies.append(latency)\n",
    "            failures += 1\n",
    "            \n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            mlflow.log_metric(f\"query_{idx}_success\", 0)\n",
    "            mlflow.log_metric(f\"query_{idx}_latency_sec\", latency)\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    total_queries = len(test_queries)\n",
    "    success_rate = (successes / total_queries) * 100\n",
    "    failure_rate = (failures / total_queries) * 100\n",
    "    \n",
    "    avg_latency = np.mean(latencies) if latencies else 0\n",
    "    min_latency = np.min(latencies) if latencies else 0\n",
    "    max_latency = np.max(latencies) if latencies else 0\n",
    "    std_latency = np.std(latencies) if latencies else 0\n",
    "    \n",
    "    avg_revenue = np.mean(predicted_revenues) if predicted_revenues else 0\n",
    "    avg_roi = np.mean(predicted_rois) if predicted_rois else 0\n",
    "    \n",
    "    # Log aggregate metrics to MLflow\n",
    "    mlflow.log_metric(\"success_rate_pct\", success_rate)\n",
    "    mlflow.log_metric(\"failure_rate_pct\", failure_rate)\n",
    "    mlflow.log_metric(\"total_successes\", successes)\n",
    "    mlflow.log_metric(\"total_failures\", failures)\n",
    "    \n",
    "    mlflow.log_metric(\"avg_latency_sec\", avg_latency)\n",
    "    mlflow.log_metric(\"min_latency_sec\", min_latency)\n",
    "    mlflow.log_metric(\"max_latency_sec\", max_latency)\n",
    "    mlflow.log_metric(\"std_latency_sec\", std_latency)\n",
    "    \n",
    "    if predicted_revenues:\n",
    "        mlflow.log_metric(\"avg_predicted_revenue_k\", avg_revenue)\n",
    "    if predicted_rois:\n",
    "        mlflow.log_metric(\"avg_predicted_roi_pct\", avg_roi)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Agent Test Metrics Summary\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Execution Metrics:\")\n",
    "    print(f\"   Total Queries:     {total_queries}\")\n",
    "    print(f\"   Successes:         {successes} ({success_rate:.1f}%)\")\n",
    "    print(f\"   Failures:          {failures} ({failure_rate:.1f}%)\")\n",
    "    print(f\"\\n‚è±Ô∏è  Latency Metrics:\")\n",
    "    print(f\"   Average:           {avg_latency:.3f}s\")\n",
    "    print(f\"   Min:               {min_latency:.3f}s\")\n",
    "    print(f\"   Max:               {max_latency:.3f}s\")\n",
    "    print(f\"   Std Dev:           {std_latency:.3f}s\")\n",
    "    \n",
    "    if predicted_revenues:\n",
    "        print(f\"\\nüí∞ Prediction Metrics:\")\n",
    "        print(f\"   Avg Revenue:       ${avg_revenue:.1f}k\")\n",
    "    if predicted_rois:\n",
    "        print(f\"   Avg ROI:           {avg_roi:.1f}%\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n‚úì Metrics logged to MLflow (Run ID: {test_run.info.run_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6196d",
   "metadata": {},
   "source": [
    "## Interactive Chat with MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat_with_agent():\n",
    "#     \"\"\"\n",
    "#     Interactive chat loop with MLflow tracking.\n",
    "#     Type 'quit' to exit.\n",
    "#     \"\"\"\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"Marketing Agent Chat (with MLflow Tracking)\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(\"Ask questions like:\")\n",
    "#     print(\"  - What if I spend 50k on digital?\")\n",
    "#     print(\"  - Predict revenue for 100k spend\")\n",
    "#     print(\"\\nType 'quit' to exit\\n\")\n",
    "    \n",
    "#     while True:\n",
    "#         user_input = input(\"You: \").strip()\n",
    "        \n",
    "#         if user_input.lower() in {'quit', 'exit', 'q'}:\n",
    "#             print(\"\\nüëã Ending chat. Check MLflow UI for tracked experiments!\")\n",
    "#             print(f\"   Run: mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\")\n",
    "#             break\n",
    "        \n",
    "#         if not user_input:\n",
    "#             continue\n",
    "        \n",
    "#         # Invoke agent\n",
    "#         result = marketing_agent.invoke({\"messages\": [HumanMessage(content=user_input)]})\n",
    "        \n",
    "#         # Display response\n",
    "#         ai_messages = [msg for msg in result[\"messages\"] if isinstance(msg, AIMessage)]\n",
    "#         if ai_messages:\n",
    "#             print(f\"\\nAgent: {ai_messages[-1].content}\\n\")\n",
    "\n",
    "# Uncomment to start interactive chat\n",
    "# chat_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7729791",
   "metadata": {},
   "source": [
    "## View MLflow Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current experiment\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"\\nüìä MLflow Experiment: {experiment.name}\")\n",
    "print(f\"üìÅ Artifact Location: {experiment.artifact_location}\")\n",
    "print(f\"üÜî Experiment ID: {experiment.experiment_id}\")\n",
    "\n",
    "# Get recent runs\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id], order_by=[\"start_time DESC\"], max_results=5)\n",
    "\n",
    "if not runs.empty:\n",
    "    print(\"\\nüìù Recent Runs:\")\n",
    "    \n",
    "    # Select columns that exist in the DataFrame\n",
    "    desired_cols = ['run_id', 'start_time', 'tags.mlflow.runName', 'metrics.r2_score']\n",
    "    available_cols = [col for col in desired_cols if col in runs.columns]\n",
    "    \n",
    "    if available_cols:\n",
    "        print(runs[available_cols].to_string(index=False))\n",
    "    else:\n",
    "        # Fallback: show basic columns\n",
    "        basic_cols = [col for col in ['run_id', 'start_time'] if col in runs.columns]\n",
    "        if basic_cols:\n",
    "            print(runs[basic_cols].to_string(index=False))\n",
    "        else:\n",
    "            print(runs.head().to_string())\n",
    "else:\n",
    "    print(\"\\nNo runs found yet.\")\n",
    "\n",
    "# print(\"\\nüåê To view in MLflow UI, run:\")\n",
    "# print(f\"   mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\")\n",
    "# print(\"   Then open: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ad4f3",
   "metadata": {},
   "source": [
    "## Batch Predictions with MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c0df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch prediction scenarios\n",
    "batch_scenarios = pd.DataFrame({\n",
    "    'scenario': ['Conservative', 'Moderate', 'Aggressive', 'Maximum'],\n",
    "    'spend_k': [40, 70, 100, 130]\n",
    "})\n",
    "\n",
    "with mlflow.start_run(run_name=\"batch_predictions\"):\n",
    "    \n",
    "    mlflow.log_param(\"batch_size\", len(batch_scenarios))\n",
    "    \n",
    "    # Make predictions\n",
    "    batch_scenarios['predicted_revenue_k'] = batch_scenarios['spend_k'].apply(predict_revenue)\n",
    "    batch_scenarios['roi_percent'] = ((batch_scenarios['predicted_revenue_k'] / batch_scenarios['spend_k']) - 1) * 100\n",
    "    \n",
    "    # Log batch results\n",
    "    for idx, row in batch_scenarios.iterrows():\n",
    "        mlflow.log_metric(f\"scenario_{row['scenario']}_spend_k\", row['spend_k'])\n",
    "        mlflow.log_metric(f\"scenario_{row['scenario']}_revenue_k\", row['predicted_revenue_k'])\n",
    "        mlflow.log_metric(f\"scenario_{row['scenario']}_roi_pct\", row['roi_percent'])\n",
    "    \n",
    "    # Save batch results\n",
    "    batch_path = \"batch_predictions.csv\"\n",
    "    batch_scenarios.to_csv(batch_path, index=False)\n",
    "    mlflow.log_artifact(batch_path)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Revenue vs Spend\n",
    "    ax1.bar(batch_scenarios['scenario'], batch_scenarios['predicted_revenue_k'], color='steelblue')\n",
    "    ax1.set_xlabel('Scenario')\n",
    "    ax1.set_ylabel('Predicted Revenue ($k)')\n",
    "    ax1.set_title('Revenue by Scenario')\n",
    "    ax1.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # ROI comparison\n",
    "    ax2.bar(batch_scenarios['scenario'], batch_scenarios['roi_percent'], color='coral')\n",
    "    ax2.set_xlabel('Scenario')\n",
    "    ax2.set_ylabel('ROI (%)')\n",
    "    ax2.set_title('ROI by Scenario')\n",
    "    ax2.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    batch_viz_path = \"batch_predictions_viz.png\"\n",
    "    plt.savefig(batch_viz_path, dpi=150)\n",
    "    mlflow.log_artifact(batch_viz_path)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüìä Batch Prediction Results:\")\n",
    "print(batch_scenarios.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3315cc74",
   "metadata": {},
   "source": [
    "## MLflow GenAI Agent Evaluation\n",
    "\n",
    "This section uses MLflow's native `mlflow.genai.evaluate()` framework to evaluate the agent with:\n",
    "- **Custom scorers** that access agent traces\n",
    "- **Structured evaluation dataset** with inputs and expectations\n",
    "- **Automatic logging** to MLflow experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2982a",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Evaluation Dataset\n",
    "\n",
    "Create evaluation dataset with inputs and expectations following MLflow's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826bfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataset with inputs, expectations, and tags\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"task\": \"What if I spend 35k on digital?\"},\n",
    "        \"expectations\": {\n",
    "            \"spend_k\": 35,\n",
    "            \"revenue_range\": (230, 240),  # Expected revenue range\n",
    "            \"routing_target\": \"Predictor\"\n",
    "        },\n",
    "        \"tags\": {\"category\": \"low_spend\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"task\": \"Predict revenue for 55k spend\"},\n",
    "        \"expectations\": {\n",
    "            \"spend_k\": 55,\n",
    "            \"revenue_range\": (290, 300),\n",
    "            \"routing_target\": \"Predictor\"\n",
    "        },\n",
    "        \"tags\": {\"category\": \"medium_spend\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"task\": \"How much revenue if we spend 85k?\"},\n",
    "        \"expectations\": {\n",
    "            \"spend_k\": 85,\n",
    "            \"revenue_range\": (323, 333),\n",
    "            \"routing_target\": \"Predictor\"\n",
    "        },\n",
    "        \"tags\": {\"category\": \"high_spend\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"task\": \"What's the expected revenue for 105k digital spend?\"},\n",
    "        \"expectations\": {\n",
    "            \"spend_k\": 105,\n",
    "            \"revenue_range\": (330, 340),\n",
    "            \"routing_target\": \"Predictor\"\n",
    "        },\n",
    "        \"tags\": {\"category\": \"very_high_spend\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"task\": \"Tell me about your capabilities\"},\n",
    "        \"expectations\": {\n",
    "            \"routing_target\": \"FINISH\",\n",
    "            \"should_not_predict\": True\n",
    "        },\n",
    "        \"tags\": {\"category\": \"off_topic\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(eval_dataset)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795667f",
   "metadata": {},
   "source": [
    "### Step 2: Define Custom Scorers\n",
    "\n",
    "Create scorers that evaluate agent behavior using traces and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c142416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import Trace, SpanType, Feedback\n",
    "from mlflow.genai import scorer\n",
    "\n",
    "# Scorer 1: Check if prediction is within expected range\n",
    "@scorer\n",
    "def revenue_accuracy(outputs, expectations) -> bool:\n",
    "    \"\"\"Check if predicted revenue is within expected range.\"\"\"\n",
    "    if \"should_not_predict\" in expectations and expectations[\"should_not_predict\"]:\n",
    "        # For off-topic queries, we expect no revenue prediction\n",
    "        return \"Marketing Prediction\" not in str(outputs)\n",
    "    \n",
    "    # Extract predicted revenue from output\n",
    "    import re\n",
    "    output_str = str(outputs)\n",
    "    revenue_match = re.search(r'Predicted Revenue:\\*\\* \\$(\\d+\\.?\\d*)k', output_str)\n",
    "    \n",
    "    if not revenue_match:\n",
    "        return False\n",
    "    \n",
    "    predicted_revenue = float(revenue_match.group(1))\n",
    "    revenue_range = expectations.get(\"revenue_range\", (0, 0))\n",
    "    \n",
    "    return revenue_range[0] <= predicted_revenue <= revenue_range[1]\n",
    "\n",
    "\n",
    "# Scorer 2: Evaluate routing using trace\n",
    "@scorer\n",
    "def correct_routing(trace: Trace, expectations: dict) -> Feedback:\n",
    "    \"\"\"Evaluate if agent routed to the correct node.\"\"\"\n",
    "    expected_target = expectations.get(\"routing_target\", \"Predictor\")\n",
    "    \n",
    "    # Search for supervisor spans in the trace\n",
    "    supervisor_spans = trace.search_spans(name=\"supervisor\")\n",
    "    \n",
    "    if not supervisor_spans:\n",
    "        return Feedback(\n",
    "            value=\"no\",\n",
    "            rationale=\"No supervisor span found in trace\"\n",
    "        )\n",
    "    \n",
    "    # Check if the supervisor routed to the expected target\n",
    "    # Look at the span attributes or messages\n",
    "    supervisor_span = supervisor_spans[0]\n",
    "    \n",
    "    # Try to extract routing decision from span events or attributes\n",
    "    routed_correctly = False\n",
    "    routing_info = \"Unknown routing\"\n",
    "    \n",
    "    # Check span events for routing information\n",
    "    if supervisor_span.events:\n",
    "        for event in supervisor_span.events:\n",
    "            if expected_target in str(event.name):\n",
    "                routed_correctly = True\n",
    "                routing_info = f\"Routed to {expected_target}\"\n",
    "                break\n",
    "    \n",
    "    # If we can't determine from events, check child spans\n",
    "    if not routed_correctly:\n",
    "        all_spans = trace.data.spans\n",
    "        for span in all_spans:\n",
    "            if span.name == \"Predictor\" and expected_target == \"Predictor\":\n",
    "                routed_correctly = True\n",
    "                routing_info = \"Found Predictor span\"\n",
    "                break\n",
    "            elif span.name != \"Predictor\" and expected_target == \"FINISH\":\n",
    "                routed_correctly = True\n",
    "                routing_info = \"Did not call Predictor\"\n",
    "                break\n",
    "    \n",
    "    value = \"yes\" if routed_correctly else \"no\"\n",
    "    rationale = f\"Expected routing to {expected_target}. {routing_info}\"\n",
    "    \n",
    "    return Feedback(value=value, rationale=rationale)\n",
    "\n",
    "\n",
    "# Scorer 3: Check latency\n",
    "@scorer\n",
    "def acceptable_latency(trace: Trace) -> Feedback:\n",
    "    \"\"\"Check if agent responded within acceptable time.\"\"\"\n",
    "    # Get trace duration in milliseconds\n",
    "    duration_ms = trace.info.execution_time_ms\n",
    "    duration_sec = duration_ms / 1000 if duration_ms else 0\n",
    "    \n",
    "    # Define acceptable latency threshold (e.g., 5 seconds)\n",
    "    threshold_sec = 5.0\n",
    "    \n",
    "    is_acceptable = duration_sec <= threshold_sec\n",
    "    value = \"yes\" if is_acceptable else \"no\"\n",
    "    rationale = f\"Response time: {duration_sec:.2f}s (threshold: {threshold_sec}s)\"\n",
    "    \n",
    "    return Feedback(value=value, rationale=rationale)\n",
    "\n",
    "\n",
    "# Scorer 4: Extract prediction error (if ground truth available)\n",
    "@scorer\n",
    "def prediction_error_pct(outputs, expectations) -> float:\n",
    "    \"\"\"Calculate prediction error percentage against ground truth.\"\"\"\n",
    "    if \"revenue_range\" not in expectations:\n",
    "        return 0.0\n",
    "    \n",
    "    # Extract predicted revenue\n",
    "    import re\n",
    "    output_str = str(outputs)\n",
    "    revenue_match = re.search(r'Predicted Revenue:\\*\\* \\$(\\d+\\.?\\d*)k', output_str)\n",
    "    \n",
    "    if not revenue_match:\n",
    "        return 100.0  # Max error if prediction not found\n",
    "    \n",
    "    predicted_revenue = float(revenue_match.group(1))\n",
    "    \n",
    "    # Use middle of expected range as ground truth\n",
    "    expected_range = expectations[\"revenue_range\"]\n",
    "    ground_truth = (expected_range[0] + expected_range[1]) / 2\n",
    "    \n",
    "    # Calculate percentage error\n",
    "    error_pct = abs(predicted_revenue - ground_truth) / ground_truth * 100\n",
    "    \n",
    "    return round(error_pct, 2)\n",
    "\n",
    "print(\"‚úì Custom scorers defined:\")\n",
    "print(\"  - revenue_accuracy: Checks if prediction is within expected range\")\n",
    "print(\"  - correct_routing: Validates agent routing decision using trace\")\n",
    "print(\"  - acceptable_latency: Ensures response time < 5s\")\n",
    "print(\"  - prediction_error_pct: Calculates prediction error percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a16408",
   "metadata": {},
   "source": [
    "### Step 3: Define Prediction Function\n",
    "\n",
    "Wrap the agent in a function that MLflow can call during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Prediction function for MLflow evaluation.\n",
    "    Takes a task/query and returns the agent's response.\n",
    "    \"\"\"\n",
    "    result = marketing_agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=task)]},\n",
    "        config={\"configurable\": {\"thread_id\": f\"eval_{hash(task)}\"}}\n",
    "    )\n",
    "    \n",
    "    # Extract the final response\n",
    "    ai_messages = [msg for msg in result[\"messages\"] if isinstance(msg, AIMessage)]\n",
    "    \n",
    "    if ai_messages:\n",
    "        # Return the last (most comprehensive) AI message\n",
    "        return ai_messages[-1].content\n",
    "    else:\n",
    "        return \"No response generated\"\n",
    "\n",
    "print(\"‚úì Prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddd059",
   "metadata": {},
   "source": [
    "### Step 4: Run MLflow GenAI Evaluation\n",
    "\n",
    "Execute the evaluation using `mlflow.genai.evaluate()` with custom scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbb042",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Running MLflow GenAI Agent Evaluation\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run evaluation with MLflow\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        revenue_accuracy,\n",
    "        correct_routing,\n",
    "        acceptable_latency,\n",
    "        prediction_error_pct\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Evaluation Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úì Evaluated {len(eval_dataset)} test cases\")\n",
    "print(f\"‚úì Results logged to MLflow experiment: {experiment_name}\")\n",
    "print(f\"\\nüìä View detailed results in MLflow UI:\")\n",
    "print(f\"   1. Run: mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   2. Open: http://localhost:5000\")\n",
    "print(f\"   3. Navigate to the latest run to see:\")\n",
    "print(f\"      - Evaluation metrics and scores\")\n",
    "print(f\"      - Agent traces for each test case\")\n",
    "print(f\"      - Scorer rationales and feedback\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ede53",
   "metadata": {},
   "source": [
    "### Key Features of MLflow GenAI Evaluation\n",
    "\n",
    "**What Makes This Different:**\n",
    "\n",
    "1. **Trace-Based Evaluation** üîç\n",
    "   - Access to agent's intermediate steps via `Trace` object\n",
    "   - Inspect routing decisions, tool calls, and execution flow\n",
    "   - Debug exactly where agent behavior differs from expectations\n",
    "\n",
    "2. **Custom Scorers with `@scorer` Decorator** ‚öôÔ∏è\n",
    "   - Create domain-specific evaluation metrics\n",
    "   - Return `Feedback` objects with rationale\n",
    "   - Automatic logging to MLflow\n",
    "\n",
    "3. **Automatic Experiment Tracking** üìä\n",
    "   - All predictions, scores, and traces logged automatically\n",
    "   - Compare evaluation runs over time\n",
    "   - No manual metric calculation needed\n",
    "\n",
    "4. **Visual Analysis in MLflow UI** üé®\n",
    "   - View traces with detailed span information\n",
    "   - Compare scorer results across test cases\n",
    "   - Identify patterns in agent failures\n",
    "\n",
    "**Next Steps:**\n",
    "- Launch MLflow UI to explore evaluation results\n",
    "- Click on individual test cases to view traces\n",
    "- Refine scorers based on evaluation insights\n",
    "- Run evaluation again after agent improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
